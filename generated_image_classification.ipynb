{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T13:41:30.313293Z",
     "iopub.status.busy": "2023-12-27T13:41:30.312392Z",
     "iopub.status.idle": "2023-12-27T13:41:30.324110Z",
     "shell.execute_reply": "2023-12-27T13:41:30.323070Z",
     "shell.execute_reply.started": "2023-12-27T13:41:30.313257Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports and constants\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import multiprocessing\n",
    "import cv2\n",
    "import PIL\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, DepthwiseConv2D, GlobalAveragePooling2D, Reshape, Rescaling, Normalization\n",
    "from keras.layers import add, multiply\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, Input, Add, AveragePooling2D, ZeroPadding2D\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.optimizers.schedules import CosineDecayRestarts, CosineDecay\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.losses import CategoricalCrossentropy\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Folder containing the train, validation, test sets, and the CSV files\n",
    "DL_FOLDER = 'project-deep-learning-data'\n",
    "\n",
    "# Train, validation and test set image folders\n",
    "TRAIN_FOLDER = 'train_images'\n",
    "VAL_FOLDER = 'val_images'\n",
    "TEST_FOLDER = 'test_images'\n",
    "KAGGLE_INPUT = ''\n",
    "\n",
    "# Paths to datasets\n",
    "DATASET_FOLDER = os.path.join(KAGGLE_INPUT, DL_FOLDER)\n",
    "TRAIN_PATH = os.path.join(KAGGLE_INPUT, DL_FOLDER, TRAIN_FOLDER)\n",
    "VAL_PATH = os.path.join(KAGGLE_INPUT, DL_FOLDER, VAL_FOLDER)\n",
    "TEST_PATH = os.path.join(KAGGLE_INPUT, DL_FOLDER, TEST_FOLDER)\n",
    "\n",
    "# Paths to the CSV files containing image-label associations \n",
    "TRAIN_LABELS_PATH = os.path.join(KAGGLE_INPUT, DL_FOLDER, 'train.csv')\n",
    "VAL_LABELS_PATH = os.path.join(KAGGLE_INPUT, DL_FOLDER, 'val.csv')\n",
    "TEST_LABELS_PATH = os.path.join(KAGGLE_INPUT, DL_FOLDER, 'test.csv')\n",
    "\n",
    "# Name of submission file\n",
    "SUBMISSION_FILE = \"submission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T13:38:33.117228Z",
     "iopub.status.busy": "2023-12-27T13:38:33.116779Z",
     "iopub.status.idle": "2023-12-27T13:38:35.488252Z",
     "shell.execute_reply": "2023-12-27T13:38:35.487336Z",
     "shell.execute_reply.started": "2023-12-27T13:38:33.117203Z"
    }
   },
   "outputs": [],
   "source": [
    "# Processing for training using GPU\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n",
    "physical_device = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(f'Device found : {physical_device}')\n",
    "tf.config.experimental.set_memory_growth(physical_device[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T13:41:32.327221Z",
     "iopub.status.busy": "2023-12-27T13:41:32.326531Z",
     "iopub.status.idle": "2023-12-27T13:41:32.331751Z",
     "shell.execute_reply": "2023-12-27T13:41:32.330736Z",
     "shell.execute_reply.started": "2023-12-27T13:41:32.327184Z"
    }
   },
   "outputs": [],
   "source": [
    "# Some constants related to model training\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 100\n",
    "IMG_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T13:41:50.738029Z",
     "iopub.status.busy": "2023-12-27T13:41:50.737628Z",
     "iopub.status.idle": "2023-12-27T13:41:50.805221Z",
     "shell.execute_reply": "2023-12-27T13:41:50.804255Z",
     "shell.execute_reply.started": "2023-12-27T13:41:50.737993Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read the train, validation and test labels from the CSV files\n",
    "def get_df_labels(df_path):\n",
    "    df = pd.read_csv(df_path)\n",
    "    if 'Class' in df.columns:\n",
    "        df['Class'] = df['Class'].astype(str)\n",
    "    return df\n",
    "    \n",
    "df_train_labels = get_df_labels(TRAIN_LABELS_PATH)\n",
    "df_val_labels = get_df_labels(VAL_LABELS_PATH)\n",
    "df_test_labels = get_df_labels(TEST_LABELS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T13:41:52.259295Z",
     "iopub.status.busy": "2023-12-27T13:41:52.258527Z",
     "iopub.status.idle": "2023-12-27T13:41:52.268988Z",
     "shell.execute_reply": "2023-12-27T13:41:52.267843Z",
     "shell.execute_reply.started": "2023-12-27T13:41:52.259260Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read an image and resize it to a specified size\n",
    "def get_image(image_path, image_size):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.resize(image, image_size)\n",
    "    return image\n",
    "\n",
    "# Read dataset based on CSV files. Used for training and validation set (as we have labels)\n",
    "def read_dataset(df_labels, folder, image_size):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for _, row in df_labels.iterrows():\n",
    "        d = row.to_dict()\n",
    "        image_name = d['Image']\n",
    "        image_label = d['Class']\n",
    "        \n",
    "        image_path = os.path.join(DATASET_FOLDER, folder, image_name)\n",
    "        data.append(get_image(image_path, (image_size, image_size)))\n",
    "        labels.append(image_label)\n",
    "    \n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels, dtype=int)\n",
    "    labels = to_categorical(labels, num_classes=NUM_CLASSES)\n",
    "    return data, labels\n",
    "\n",
    "# Read test set based on the test.csv file.\n",
    "# Returns the dataset as a numpy array and the name of the test images.\n",
    "def read_test_set(df_labels, folder, image_size):\n",
    "    data = []\n",
    "    filenames = []\n",
    "    for _, row in df_labels.iterrows():\n",
    "        d = row.to_dict()\n",
    "        image_name = d['Image']\n",
    "        filenames.append(image_name)\n",
    "        \n",
    "        image_path = os.path.join(DATASET_FOLDER, folder, image_name)\n",
    "        data.append(get_image(image_path, (image_size, image_size)))\n",
    "        \n",
    "    return np.array(data), filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T09:20:51.779185Z",
     "iopub.status.busy": "2023-12-27T09:20:51.778755Z",
     "iopub.status.idle": "2023-12-27T09:21:14.885889Z",
     "shell.execute_reply": "2023-12-27T09:21:14.885036Z",
     "shell.execute_reply.started": "2023-12-27T09:20:51.779151Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read train, validation and test set\n",
    "X_train, y_train = read_dataset(df_train_labels, TRAIN_FOLDER, 64)\n",
    "X_val, y_val = read_dataset(df_val_labels, VAL_FOLDER, 64)\n",
    "X_test, test_filenames = read_test_set(df_test_labels, TEST_FOLDER, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T13:41:54.250192Z",
     "iopub.status.busy": "2023-12-27T13:41:54.249427Z",
     "iopub.status.idle": "2023-12-27T13:41:54.255637Z",
     "shell.execute_reply": "2023-12-27T13:41:54.254579Z",
     "shell.execute_reply.started": "2023-12-27T13:41:54.250159Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generators for augmentation and standardization\n",
    "datagen = ImageDataGenerator(rotation_range=40, width_shift_range=0.2,\n",
    "                             height_shift_range=0.2, rescale=1./255.0, \n",
    "\t\t\t\t\t\t\t shear_range=0.2, zoom_range=0.2,\n",
    "                             horizontal_flip=True, fill_mode='nearest')\n",
    "\n",
    "val_test_datagen = ImageDataGenerator(rescale=1./255)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T09:21:22.318141Z",
     "iopub.status.busy": "2023-12-27T09:21:22.317255Z",
     "iopub.status.idle": "2023-12-27T09:21:22.655177Z",
     "shell.execute_reply": "2023-12-27T09:21:22.654214Z",
     "shell.execute_reply.started": "2023-12-27T09:21:22.318109Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create generators for train, validation and test set\n",
    "train_generator = datagen.flow(X_train, y_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_generator = val_test_datagen.flow(X_val, y_val, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_generator = val_test_datagen.flow(X_test, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T13:41:56.288673Z",
     "iopub.status.busy": "2023-12-27T13:41:56.287758Z",
     "iopub.status.idle": "2023-12-27T13:41:56.293096Z",
     "shell.execute_reply": "2023-12-27T13:41:56.291792Z",
     "shell.execute_reply.started": "2023-12-27T13:41:56.288641Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to get number of steps per epoch (based on batch size)\n",
    "def get_steps_per_epoch(generator):\n",
    "    return len(generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset train and validation sets\n",
    "def reset_generators(train_generator, val_generator):\n",
    "    train_generator.reset()\n",
    "    val_generator.reset()\n",
    "\n",
    "# Function to fit model. Fitting uses a checkpoint to save the model that has\n",
    "# the best accuracy on the validation set (which will be saved to a h5 file).\n",
    "def fit_model(model, train_generator, val_generator, num_epochs=40, checkpoint_name='best_model.h5'):\n",
    "    reset_generators(train_generator, val_generator)\n",
    "    \n",
    "    checkpoint_callback = ModelCheckpoint(checkpoint_name, save_best_only=True, monitor='val_accuracy', mode='max', verbose=1)\n",
    "    model.fit(train_generator,\n",
    "              validation_data=val_generator,\n",
    "              epochs=num_epochs,\n",
    "              workers=multiprocessing.cpu_count(),\n",
    "              callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T13:32:53.561528Z",
     "iopub.status.busy": "2023-12-27T13:32:53.561068Z",
     "iopub.status.idle": "2023-12-27T13:32:53.582302Z",
     "shell.execute_reply": "2023-12-27T13:32:53.580674Z",
     "shell.execute_reply.started": "2023-12-27T13:32:53.561481Z"
    }
   },
   "outputs": [],
   "source": [
    "# CNN1 model architecture. \n",
    "# Can be trained using either Adam or SGD (with the default) values.\n",
    "def cnn_1_model(use_adam=True):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, (3, 3), input_shape=(IMG_SIZE, IMG_SIZE, 3), activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(16, (3, 3), activation=\"relu\"))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation=\"relu\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(NUM_CLASSES, activation=\"softmax\"))\n",
    "    \n",
    "    if use_adam:\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    else:\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models for CNN1 using both Adam and SGD and train them\n",
    "cnn_1_adam = cnn_1_model(True)\n",
    "cnn_1_sgd = cnn_1_model(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model(cnn_1_adam, train_generator, val_generator, 60, 'cnn_1_adam_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model(cnn_1_sgd, train_generator, val_generator, 60, 'cnn_1_sgd_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN2 model architecture.\n",
    "# Can be trained either using the default Adam optimizer or using Cosine restarts\n",
    "# (initial learning rate will be 1e-3 and final learning rate will be 1e-5)\n",
    "def cnn_2_model(num_epochs, use_default_optimizer=False):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(NUM_CLASSES, activation=\"softmax\"))\n",
    "    \n",
    "    steps_per_epoch = get_steps_per_epoch(train_generator)\n",
    "    initial_learning_rate = 0.01\n",
    "    final_learning_rate = 0.00001\n",
    "    alpha = final_learning_rate / initial_learning_rate\n",
    "    \n",
    "    total_steps = steps_per_epoch * num_epochs\n",
    "    first_decay_steps = total_steps * 0.33\n",
    "    \n",
    "    lr_schedule = CosineDecayRestarts(initial_learning_rate=initial_learning_rate, alpha=alpha, first_decay_steps=first_decay_steps, t_mul=1.0)\n",
    "    \n",
    "    if use_default_optimizer:\n",
    "        model.compile(loss='categorical_crossentropy', \n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "    else:\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=Adam(learning_rate=lr_schedule),\n",
    "                      metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CNN2 models and train them\n",
    "cnn_2_default = cnn_2_model(True)\n",
    "cnn_2_cosine_restarts = cnn_2_model(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model(cnn_2_default, train_generator, val_generator, 60, 'cnn_2_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model(cnn_2_cosine_restarts, train_generator, val_generator, 60, 'cnn_2_weights_cos.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T13:41:24.223272Z",
     "iopub.status.busy": "2023-12-27T13:41:24.222397Z",
     "iopub.status.idle": "2023-12-27T13:41:24.236221Z",
     "shell.execute_reply": "2023-12-27T13:41:24.235099Z",
     "shell.execute_reply.started": "2023-12-27T13:41:24.223236Z"
    }
   },
   "outputs": [],
   "source": [
    "# CNN3 model architecture. \n",
    "# Model is compiled either using Adam optimizer or SGD Nesterov with 0.9 momentum.\n",
    "# Both are using a Cosine Decay Scheduler with 10 warmup epochs.\n",
    "# Also supports a custom activation function and label smoothing.\n",
    "def cnn_3_model(warmup_epochs=10, use_adam=True, initial_learning_rate=0.01, final_learning_rate=0.00001, \n",
    "                activation='relu', label_smoothing=0):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(256, (3, 3), activation=activation, input_shape=(64, 64, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(512, (3, 3), activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(512, (3, 3), activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(1024, (3, 3), activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(1024, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(1024, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(512, activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Dense(NUM_CLASSES, activation=\"softmax\"))\n",
    "    \n",
    "    steps_per_epoch = get_steps_per_epoch(train_generator)\n",
    "    alpha = final_learning_rate / initial_learning_rate\n",
    "\n",
    "    first_decay_steps = steps_per_epoch * 180\n",
    "    warmup_steps = steps_per_epoch * warmup_epochs\n",
    "    lr_schedule = CosineDecay(0.0, first_decay_steps, alpha, warmup_target=initial_learning_rate, warmup_steps=warmup_steps)\n",
    "\n",
    "    if use_adam:\n",
    "        optimizer = Adam(learning_rate=lr_schedule, weight_decay=0.001)\n",
    "        print(\"Using Adam optimzier\")\n",
    "    else:\n",
    "        optimizer = SGD(learning_rate=lr_schedule, weight_decay=0.001, momentum=0.9, nesterov=True)\n",
    "        print(\"Using SGD optimizer\")\n",
    "    \n",
    "    model.compile(loss=CategoricalCrossentropy(label_smoothing=label_smoothing), optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_3_cosine_swish = cnn_3_model(warmup_epochs=10, use_adam=True, initial_learning_rate=0.01, final_learning_rate=0.00001, activation='swish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model(cnn_3_cosine_swish, train_generator, val_generator, 200, 'cnn_3_adam_swish_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_3_cosine_sgd = cnn_3_model(warmup_epochs=10, use_adam=False, initial_learning_rate=0.01, final_learning_rate=0.00001, activation='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model(cnn_3_cosine_sgd, train_generator, val_generator, 200, 'cnn_3_cosine_sgd_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN4 model architecture.\n",
    "# Compiled using Adam optimizer with Cosine Decay learning rate scheduler and 10 warmup epochs.\n",
    "def cnn_4_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', padding='same', input_shape=(64, 64, 3)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(256, (3, 3), padding=\"same\", activation='relu'))\n",
    "    model.add(Conv2D(256, (3, 3), padding=\"same\", activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(1024, (3, 3), activation='relu'))\n",
    "    model.add(Conv2D(1024, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(1024, (3, 3), activation='relu'))\n",
    "    model.add(Conv2D(1024, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Dense(NUM_CLASSES, activation=\"softmax\"))\n",
    "    \n",
    "    steps_per_epoch = get_steps_per_epoch(train_generator)\n",
    "    initial_learning_rate = 0.005\n",
    "    final_learning_rate = 0.00001\n",
    "    alpha = final_learning_rate / initial_learning_rate\n",
    "\n",
    "    first_decay_steps = steps_per_epoch * 80\n",
    "    warmup_steps = steps_per_epoch * 10\n",
    "    lr_schedule = CosineDecay(0.0, first_decay_steps, alpha, warmup_target=initial_learning_rate, warmup_steps=warmup_steps)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(learning_rate=lr_schedule, weight_decay=0.001),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identity block for the ResNet. Input and output activations have the same dimensions.\n",
    "def identity_block(input, filters, conv_kernel_initializer='glorot_uniform', activation='relu'):\n",
    "    filter1, filter2, filter3 = filters\n",
    "    X_residual = input\n",
    "\n",
    "    # First layer\n",
    "    X = Conv2D(filters=filter1, kernel_size=(1, 1), kernel_initializer=conv_kernel_initializer)(input)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation(activation)(X)\n",
    "\n",
    "    # Second layer\n",
    "    X = Conv2D(filters=filter2, kernel_size=(3, 3), padding='same', kernel_initializer=conv_kernel_initializer)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation(activation)(X)\n",
    "\n",
    "    # Third layer\n",
    "    X = Conv2D(filters=filter3, kernel_size=(1, 1), kernel_initializer=conv_kernel_initializer)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "\n",
    "    # Add skip-connection\n",
    "    X = Add()([X, X_residual])\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation(activation)(X)\n",
    "\n",
    "    return X\n",
    "\n",
    "# Residual block for the ResNet. Input and output activations can have a different number of dimensions.\n",
    "def residual_block(input, filters, strides, conv_kernel_initializer='glorot_uniform', activation='relu'):\n",
    "    filter1, filter2, filter3 = filters\n",
    "    X_residual = input\n",
    "\n",
    "    # First layer\n",
    "    X = Conv2D(filters=filter1, kernel_size=(1, 1), strides=strides, kernel_initializer=conv_kernel_initializer)(input)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation(activation)(X)\n",
    "\n",
    "    # Second layer\n",
    "    X = Conv2D(filters=filter2, kernel_size=(3, 3), padding='same', kernel_initializer=conv_kernel_initializer)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation(activation)(X)\n",
    "\n",
    "    # Third layer\n",
    "    X = Conv2D(filters=filter3, kernel_size=(1, 1), kernel_initializer=conv_kernel_initializer)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "\n",
    "    # Compute skip-connection\n",
    "    X_residual = Conv2D(filters=filter3, kernel_size=(1, 1), strides=strides, kernel_initializer=conv_kernel_initializer)(X_residual)\n",
    "    X_residual = BatchNormalization()(X_residual)\n",
    "\n",
    "    # Add skip-connection\n",
    "    X = Add()([X, X_residual])\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation(activation)(X)\n",
    "\n",
    "    return X\n",
    "\n",
    "# CNN ResNet model.\n",
    "# Can change the convolutional kernel initializer, the activation function,\n",
    "# whether to use Adam or SGD, the initial and final learning rates, add label smoothing\n",
    "# or not, and the weight decay value.\n",
    "# Model is compiled using Cosine Decay scheduler\n",
    "def cnn_residual_model(image_size=64, warmup_epochs=0, \n",
    "                       conv_kernel_initializer='glorot_uniform', activation='relu', use_adam=True, \n",
    "                       initial_learning_rate=0.01, final_learning_rate=0.00001,\n",
    "                       label_smoothing=0, weight_decay=0.001):\n",
    "    X_input = Input((image_size, image_size, 3))\n",
    "\n",
    "    X = ZeroPadding2D(padding=(3, 3))(X_input)\n",
    "\n",
    "    X = Conv2D(filters=128, kernel_size=(7, 7), strides=(2, 2),\n",
    "               kernel_initializer=conv_kernel_initializer)(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Activation(activation)(X)\n",
    "\n",
    "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "\n",
    "    # First layer of the ResNet\n",
    "    X = residual_block(X, filters=[128, 128, 256], strides=(1, 1), activation=activation, conv_kernel_initializer=conv_kernel_initializer)\n",
    "    X = identity_block(X, filters=[128, 128, 256], activation=activation, conv_kernel_initializer=conv_kernel_initializer)\n",
    "    X = identity_block(X, filters=[128, 128, 256], activation=activation, conv_kernel_initializer=conv_kernel_initializer)\n",
    "    X = identity_block(X, filters=[128, 128, 256], activation=activation, conv_kernel_initializer=conv_kernel_initializer)\n",
    "\n",
    "    # Second layer of the ResNet\n",
    "    X = residual_block(X, filters=[128, 128, 512], strides=(2, 2), activation=activation, conv_kernel_initializer=conv_kernel_initializer)\n",
    "    X = identity_block(X, filters=[128, 128, 512], activation=activation, conv_kernel_initializer=conv_kernel_initializer)\n",
    "    X = identity_block(X, filters=[128, 128, 512], activation=activation, conv_kernel_initializer=conv_kernel_initializer)\n",
    "    X = identity_block(X, filters=[128, 128, 512], activation=activation, conv_kernel_initializer=conv_kernel_initializer)\n",
    "\n",
    "    # Third layer\n",
    "    X = residual_block(X, filters=[256, 256, 1024], strides=(2, 2), activation=activation, conv_kernel_initializer=conv_kernel_initializer)\n",
    "    X = identity_block(X, filters=[256, 256, 1024], activation=activation, conv_kernel_initializer=conv_kernel_initializer)\n",
    "    X = identity_block(X, filters=[256, 256, 1024], activation=activation, conv_kernel_initializer=conv_kernel_initializer)\n",
    "    X = identity_block(X, filters=[256, 256, 1024], activation=activation, conv_kernel_initializer=conv_kernel_initializer)\n",
    "    X = identity_block(X, filters=[256, 256, 1024], activation=activation, conv_kernel_initializer=conv_kernel_initializer)\n",
    "    X = identity_block(X, filters=[256, 256, 1024], activation=activation, conv_kernel_initializer=conv_kernel_initializer)\n",
    "\n",
    "    # Fourth layer\n",
    "    X = residual_block(X, filters=[512, 512, 2048], strides=(2, 2), activation=activation, conv_kernel_initializer=conv_kernel_initializer)\n",
    "    X = identity_block(X, filters=[512, 512, 2048], activation=activation, conv_kernel_initializer=conv_kernel_initializer)\n",
    "    X = identity_block(X, filters=[512, 512, 2048], activation=activation, conv_kernel_initializer=conv_kernel_initializer)\n",
    "\n",
    "    X = AveragePooling2D(pool_size=(2, 2))(X)\n",
    "\n",
    "    # Output layer\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(512, activation=activation, kernel_initializer=glorot_uniform(seed=42))(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "\n",
    "    X = Dense(512, activation=activation, kernel_initializer=glorot_uniform(seed=42))(X)\n",
    "    X = BatchNormalization()(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "\n",
    "    X = Dense(NUM_CLASSES, activation='softmax', kernel_initializer=glorot_uniform(seed=42))(X)\n",
    "    model = Model(inputs=X_input, outputs=X)\n",
    "    \n",
    "    steps_per_epoch = get_steps_per_epoch(train_generator)\n",
    "    alpha = final_learning_rate / initial_learning_rate\n",
    "\n",
    "    first_decay_steps = steps_per_epoch * 180\n",
    "    warmup_steps = steps_per_epoch * warmup_epochs\n",
    "    lr_schedule = CosineDecay(0.0, first_decay_steps, alpha, warmup_target=initial_learning_rate, warmup_steps=warmup_steps)\n",
    "\n",
    "    if use_adam:\n",
    "        optimizer = Adam(learning_rate=lr_schedule, weight_decay=weight_decay)\n",
    "        print(\"Using Adam optimzier\")\n",
    "    else:\n",
    "        optimizer = SGD(learning_rate=lr_schedule, weight_decay=weight_decay, momentum=0.9, nesterov=True)\n",
    "        print(\"Using SGD optimizer\")\n",
    "    \n",
    "    model.compile(loss=CategoricalCrossentropy(label_smoothing=label_smoothing), optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_res_glorot = cnn_residual_model(64, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model(cnn_res_glorot, train_generator, val_generator, 200, 'cnn_res_glorot_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_res_glorot_smoothing_02 = cnn_residual_model(64, 10, label_smoothing=0.2)\n",
    "fit_model(cnn_res_glorot_smoothing_02, train_generator, val_generator, 200, 'cnn_res_glorot_smoothing_02_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_res_glorot_smoothing = cnn_residual_model(64, 10, label_smoothing=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model(cnn_res_glorot_smoothing, train_generator, val_generator, 200, 'cnn_res_glorot_smoothing_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_res_he = cnn_residual_model(64, 10, 'he_normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T13:34:49.907361Z",
     "iopub.status.busy": "2023-12-27T13:34:49.906961Z",
     "iopub.status.idle": "2023-12-27T13:34:49.915621Z",
     "shell.execute_reply": "2023-12-27T13:34:49.914368Z",
     "shell.execute_reply.started": "2023-12-27T13:34:49.907331Z"
    }
   },
   "outputs": [],
   "source": [
    "fit_model(cnn_res_he, train_generator, val_generator, 200, 'cnn_res_he_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_res_sgd = cnn_residual_model(64, 10, use_adam=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model(cnn_res_sgd, train_generator, val_generator, 200, 'cnn_res_sgd_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_res_swish = cnn_residual_model(64, 10, use_adam=True, activation='swish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model(cnn_res_swish, train_generator, val_generator, 200, 'cnn_res_swish_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-27T13:24:35.844789Z",
     "iopub.status.busy": "2023-12-27T13:24:35.843825Z",
     "iopub.status.idle": "2023-12-27T13:24:35.852326Z",
     "shell.execute_reply": "2023-12-27T13:24:35.851183Z",
     "shell.execute_reply.started": "2023-12-27T13:24:35.844717Z"
    }
   },
   "outputs": [],
   "source": [
    "# Method for generating the submission file \n",
    "def generate_predictions(model, test_generator, test_filenames, checkpoint_name='best_model.h5', checkpoint_folder=''):\n",
    "    test_generator.reset()\n",
    "    model.load_weights(os.path.join(checkpoint_folder, checkpoint_name))\n",
    "    pred = model.predict(test_generator, verbose=1)\n",
    "    predictions = np.argmax(pred, axis=1)\n",
    "    \n",
    "    results = pd.DataFrame({\"Image\": test_filenames,\n",
    "                            \"Class\": predictions})\n",
    "\n",
    "    results.to_csv(SUBMISSION_FILE, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_generator.reset()\n",
    "\n",
    "cnn_res = cnn_residual_model(64, conv_kernel_initializer='he_normal')\n",
    "cnn_res.load_weights(\"Model_weights/cnn_res_weights.h5\")\n",
    "print(cnn_res.evaluate(val_generator))\n",
    "generate_predictions(cnn_res, test_generator, test_filenames, 'cnn_res_weights.h5', 'Model_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_3_cosine = cnn_3_model(0)\n",
    "cnn_3_cosine.load_weights(\"Model_weights/cnn_3_cosine_weights.h5\")\n",
    "val_generator.reset()\n",
    "print(cnn_3_cosine.evaluate(val_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model(cnn_3_cosine, train_generator, val_generator, 150, 'cnn_3_cosine_2_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_generator.reset()\n",
    "cnn_3_cosine.load_weights(\"cnn_3_cosine_2_weights.h5\")\n",
    "print(cnn_3_cosine.evaluate(val_generator))\n",
    "generate_predictions(cnn_3_cosine, test_generator, test_filenames, 'cnn_3_cosine_2_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load several trained models and evaluate them on the validation set\n",
    "# Following are only models which I considered adding to the ensemble.\n",
    "val_generator.reset()\n",
    "model_cnn3_cosine_2 = cnn_3_model()\n",
    "model_cnn3_cosine_2.load_weights(\"Model_weights/cnn_3_cosine_2_weights.h5\")\n",
    "print(model_cnn3_cosine_2.evaluate(val_generator))\n",
    "\n",
    "val_generator.reset()\n",
    "model_cnn3_cosine_1 = cnn_3_model()\n",
    "model_cnn3_cosine_1.load_weights(\"Model_weights/cnn_3_cosine_weights.h5\")\n",
    "print(model_cnn3_cosine_1.evaluate(val_generator))\n",
    "\n",
    "val_generator.reset()\n",
    "model_cnn3_simple = cnn_3_model()\n",
    "model_cnn3_simple.load_weights(\"Model_weights/cnn_3_weights.h5\")\n",
    "print(model_cnn3_simple.evaluate(val_generator))\n",
    "\n",
    "val_generator.reset()\n",
    "model_cnn3_cosine_swish = cnn_3_model(warmup_epochs=10, use_adam=True, initial_learning_rate=0.01, final_learning_rate=0.00001, activation='swish')\n",
    "model_cnn3_cosine_swish.load_weights(\"Model_weights/cnn_3_adam_swish_weights.h5\")\n",
    "print(model_cnn3_cosine_swish.evaluate(val_generator))\n",
    "\n",
    "val_generator.reset()\n",
    "model_cnn3_cosine_sgd = cnn_3_model(warmup_epochs=10, use_adam=False, initial_learning_rate=0.01, final_learning_rate=0.00001, activation='relu')\n",
    "model_cnn3_cosine_sgd.load_weights(\"Model_weights/cnn_3_cosine_sgd_weights.h5\")\n",
    "print(model_cnn3_cosine_sgd.evaluate(val_generator))\n",
    "\n",
    "val_generator.reset()\n",
    "model_cnn4 = cnn_4_model()\n",
    "model_cnn4.load_weights(\"Model_weights/cnn_4_weights.h5\")\n",
    "print(model_cnn4.evaluate(val_generator))\n",
    "\n",
    "val_generator.reset()\n",
    "model_res = cnn_residual_model(64)\n",
    "model_res.load_weights(\"Model_weights/cnn_res_glorot_weights.h5\")\n",
    "print(model_res.evaluate(val_generator))\n",
    "\n",
    "val_generator.reset()\n",
    "model_res_he = cnn_residual_model(64)\n",
    "model_res_he.load_weights(\"Model_weights/cnn_res_he_weights.h5\")\n",
    "print(model_res_he.evaluate(val_generator))\n",
    "\n",
    "val_generator.reset()\n",
    "model_res_sgd = cnn_residual_model(64, 10, use_adam=False)\n",
    "model_res_sgd.load_weights(\"Model_weights/cnn_res_sgd_weights.h5\")\n",
    "print(model_res_sgd.evaluate(val_generator))\n",
    "\n",
    "val_generator.reset()\n",
    "model_res_swish = cnn_residual_model(64, 10, use_adam=True, activation='swish')\n",
    "model_res_swish.load_weights(\"Model_weights/cnn_res_swish_weights.h5\")\n",
    "print(model_res_swish.evaluate(val_generator))\n",
    "\n",
    "val_generator.reset()\n",
    "model_res_smoothing = cnn_residual_model(64)\n",
    "model_res_smoothing.load_weights(\"Model_weights/cnn_res_glorot_smoothing_weights.h5\")\n",
    "print(model_res_smoothing.evaluate(val_generator))\n",
    "\n",
    "val_generator.reset()\n",
    "model_res_smoothing_02 = cnn_residual_model(64)\n",
    "model_res_smoothing_02.load_weights(\"Model_weights/cnn_res_glorot_smoothing_02_weights.h5\")\n",
    "print(model_res_smoothing_02.evaluate(val_generator))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other models which I have not added to ensembles due to low metrics\n",
    "model_cnn_2 = cnn_2_model(50)\n",
    "model_cnn_2.load_weights(\"Model_weights/cnn_2_weights.h5\")\n",
    "\n",
    "model_cnn_1_sgd = cnn_1_model()\n",
    "model_cnn_1_sgd.load_weights(\"Model_weights/cnn_1_sgd_weights.h5\")\n",
    "\n",
    "model_cnn_1 = cnn_1_model()\n",
    "model_cnn_1.load_weights(\"Model_weights/cnn_1_adam_weights.h5\")\n",
    "\n",
    "model_cnn_2_cos = cnn_2_model(50)\n",
    "model_cnn_2_cos.load_weights(\"Model_weights/cnn_2_weights_cos.h5\")\n",
    "\n",
    "model_cnn_1 = cnn_1_model()\n",
    "model_cnn_1.load_weights(\"Model_weights/cnn_1_adam_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate submission file for the ensemble, based on a list of trained models\n",
    "def generate_predictions_ensemble(models, test_generator, test_filenames):\n",
    "\ttest_generator.reset()\n",
    "\tpreds = []\n",
    "\tfor model in models:\n",
    "\t\ttest_generator.reset()\n",
    "\t\tpred = model.predict(test_generator, verbose=1)\n",
    "\t\tpreds.append(pred)\n",
    "\t\n",
    "\tpreds = np.array(preds)\n",
    "\tpreds_sum = np.sum(preds, axis=0)\n",
    "\tpredictions = np.argmax(preds_sum, axis=1)\n",
    "\n",
    "\tresults = pd.DataFrame({\"Image\": test_filenames,\n",
    "                            \"Class\": predictions})\n",
    "\tresults.to_csv(SUBMISSION_FILE, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First model ensemble tried\n",
    "models_ensemble_1 = [model_cnn3_cosine_1, model_cnn3_cosine_2, model_cnn3_cosine_swish,\n",
    "\t\t\t\t\t model_res, model_res_he, model_res_sgd, \n",
    "\t\t\t\t\t model_res_smoothing, model_res_smoothing_02, model_res_swish]\n",
    "\n",
    "generate_predictions_ensemble(models_ensemble_1, test_generator, test_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second model ensemble tried\n",
    "models_ensemble_2 = [model_res, model_res_he, model_res_sgd, \n",
    "\t\t\t\t\t model_res_smoothing, model_res_smoothing_02, \n",
    "\t\t\t\t\t model_res_swish]\n",
    "\n",
    "generate_predictions_ensemble(models_ensemble_2, test_generator, test_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics and Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification report for a model\n",
    "def get_metrics(model, generator, y_true):\n",
    "    generator.reset()\n",
    "    pred = model.predict(generator, verbose=1)\n",
    "    predictions = np.argmax(pred, axis=1)\n",
    "    true_labels = np.argmax(y_true, axis=1)\n",
    "    print(classification_report(true_labels, predictions, digits=3))\n",
    "\n",
    "# Generate classification report for a model ensemble\n",
    "def get_metrics_ensemble(models, generator, y_true):\n",
    "    generator.reset()\n",
    "    preds = []\n",
    "    for model in models:\n",
    "       generator.reset()\n",
    "       pred = model.predict(generator, verbose=1)\n",
    "       preds.append(pred)\n",
    "\n",
    "    preds = np.array(preds)\n",
    "    preds_sum = np.sum(preds, axis=0)\n",
    "    predictions = np.argmax(preds_sum, axis=1)\n",
    "    true_labels = np.argmax(y_true, axis=1)\n",
    "    print(classification_report(true_labels, predictions, digits=3))\n",
    "\n",
    "# Create confusion matrix image for a model\n",
    "def get_confusion_matrix(model, generator, y_true, cm_img_name):\n",
    "    generator.reset()\n",
    "    pred = model.predict(generator, verbose=1)\n",
    "    predictions = np.argmax(pred, axis=1)\n",
    "    true_labels = np.argmax(y_true, axis=1)\n",
    "\n",
    "    plt.figure(figsize=(35, 35))\n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "    ax = plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, fmt='g', ax=ax)\n",
    "\n",
    "    # labels, title and ticks\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    plt.savefig(cm_img_name, bbox_inches='tight')\n",
    "\n",
    "# Create confusion matrix for a model ensemble\n",
    "def get_confusion_matrix_ensemble(models, generator, y_true, cm_img_name):\n",
    "    generator.reset()\n",
    "    preds = []\n",
    "    for model in models:\n",
    "       generator.reset()\n",
    "       pred = model.predict(generator, verbose=1)\n",
    "       preds.append(pred)\n",
    "\n",
    "    preds = np.array(preds)\n",
    "    preds_sum = np.sum(preds, axis=0)\n",
    "    predictions = np.argmax(preds_sum, axis=1)\n",
    "    true_labels = np.argmax(y_true, axis=1)\n",
    "\n",
    "    plt.figure(figsize=(35, 35))\n",
    "    cm = confusion_matrix(true_labels, predictions)\n",
    "    ax = plt.subplot()\n",
    "    sns.heatmap(cm, annot=True, fmt='g', ax=ax)\n",
    "\n",
    "    # labels, title and ticks\n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    ax.set_title('Confusion Matrix')\n",
    "    plt.savefig(cm_img_name, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(model_cnn3_cosine_sgd, val_generator, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(model_res_smoothing, val_generator, y_val)\n",
    "get_confusion_matrix(model_res_smoothing, val_generator, y_val, \"resnet_smoothing_cm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(model_res_smoothing_02, val_generator, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(model_cnn3_cosine_swish, val_generator, y_val)\n",
    "get_confusion_matrix(model_cnn3_cosine_swish, val_generator, y_val, \"cnn3_cosine_swish_cm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(model_res_swish, val_generator, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(model_res_sgd, val_generator, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(model_res_he, val_generator, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(model_res, val_generator, y_val)\n",
    "get_confusion_matrix(model_res, val_generator, y_val, \"resnet_glorot_cm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(model_cnn4, val_generator, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(model_cnn3_simple, val_generator, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(model_cnn3_cosine_1, val_generator, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(model_cnn3_cosine_2, val_generator, y_val)\n",
    "get_confusion_matrix(model_cnn3_cosine_2, val_generator, y_val, \"cnn3_cos_2_cm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(model_cnn_2, val_generator, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(model_cnn_2_cos, val_generator, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(model_cnn_1, val_generator, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(model_cnn_1_sgd, val_generator, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics_ensemble(models_ensemble_1, val_generator, y_val)\n",
    "get_confusion_matrix_ensemble(models_ensemble_1, val_generator, y_val, \"ensemble_1_cm\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4223974,
     "sourceId": 7284399,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30626,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
